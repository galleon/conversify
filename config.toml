[agent]
env_file = ".env.local"
instructions_file = "conversify/prompts/llm.txt"
greeting = "Hey! How are you doing today?"
goodbye = "Goodbye! Have a great day!"
default_participant_identity = "identity-qfXx"
use_eou = true                                 # livekit turn detection
use_avatar = true                              # livekit turn detection
use_background_noise_removal = true            # uses Krisp BVC noise cancellation
use_background_audio = false                   # plays office background audio while agent speaks
allow_interruptions = true                     # reset tts on user interruption

[stt]
# Select the STT provider to use. Can be "local" or "openai".
provider = "openai"

# Configuration for the local STT provider (faster-whisper)
[stt.local]
language = "en"
model = "deepdml/faster-whisper-large-v3-turbo-ct2"
device = "cpu"
compute_type = "auto"                          # "int8_float16"
model_cache_directory = "conversify/data/models_cache"
warmup_audio = "conversify/data/warmup_audio.wav"

# Configuration for the OpenAI STT provider
[stt.openai]
api_key = "YOUR_OPENAI_API_KEY" # Replace with your key or set in .env.local
language = "en"
model = "whisper-1"

# [llm]
# The LLM (Large Language Model) service is configured to use a hosted
# OpenAI-compatible API. The `base_url` should point to your service provider's
# API endpoint. The `api_key` should be stored in the .env.local file.
[llm]
base_url = "https://api.openai.com/v1"
api_key = "YOUR_OPENAI_API_KEY" # Replace with your key or set in .env.local
model = "gpt-4o"
temperature = 0.4
parallel_tool_calls = false
tool_choice = "auto"

# [tts.kokoro]
# The TTS (Text-to-Speech) service is configured to use a hosted
# OpenAI-compatible API. Although the section is named `kokoro`, it uses a
# standard OpenAI client and can be pointed to any compatible service.
# The `api_key` should be stored in the .env.local file.
[tts.kokoro]
base_url = "https://api.openai.com/v1"
api_key = "YOUR_OPENAI_API_KEY" # Replace with your key or set in .env.local
model = "tts-1"
voice = "alloy" # OpenAI voices: alloy, echo, fable, onyx, nova, shimmer
speed = 1.0

[vad]
min_speech_duration = 0.20      # Minimum duration (s) for speech detection
min_silence_duration = 0.40     # Minimum silence duration (s) to detect end of speech
prefix_padding_duration = 0.5   # Padding duration (s) before detected speech
max_buffered_speech = 60.0      # Maximum buffered speech duration
activation_threshold = 0.5      # Threshold for VAD
force_cpu = false               # Force VAD to run on CPU
sample_rate = 16000

[vision]
use = true
video_frame_interval = 0.2

[memory]
use = false
dir = "conversify/data/memory_store"
load_last_n = 6

[embedding]
vllm_model_name = "mxbai-embed-large"

[worker]
job_memory_warn_mb = 1900
load_threshold = 1.0
job_memory_limit_mb = 10000

[logging]
level = "DEBUG"
file = "logs/app.log"
